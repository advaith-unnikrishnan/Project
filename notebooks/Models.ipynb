{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zvuFzbcSVD0E",
        "outputId": "a7063aea-3e42-4e7a-b4c5-49577031fa6d"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "GgB9RQilKsG2"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "import matplotlib.pyplot as plt\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LinearRegression, Lasso, Ridge\n",
        "from sklearn.metrics import mean_squared_error as mse\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "import lightgbm as lgb \n",
        "from keras.models import Sequential, Model\n",
        "from keras.layers import Dense, Embedding, LSTM, Dropout, GRU, Conv1D, MaxPooling1D, Input, concatenate\n",
        "from keras.callbacks import TensorBoard, EarlyStopping, ModelCheckpoint"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mno2Om6xKsG-"
      },
      "source": [
        "## 1. Reading the data files"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "P7JtIwNxKsHG"
      },
      "outputs": [],
      "source": [
        "train = pd.read_pickle('features.pkl')\n",
        "predict = pd.read_csv('/content/drive/MyDrive/Final Project/Train.csv')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "77AdXddhKsHL"
      },
      "source": [
        "## 2. Set up training and testing sets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "FrfYTVYuKsHN"
      },
      "outputs": [],
      "source": [
        "feature_cols = ['sum','mean','elevation','poly_rainfall','poly_elevation',\n",
        "                'elev_diff','rainfall_diff','water_count','water_dist']\n",
        "features = train[feature_cols]\n",
        "target = train['target']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "vmFYJDefKsHR"
      },
      "outputs": [],
      "source": [
        "# train-test split (80-20)\n",
        "x_train, x_val, y_train, y_val = train_test_split(features, target, test_size=0.2, random_state=42)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YB6NDkI3KsHX"
      },
      "source": [
        "## 3. Define loss function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "9plHSurhKsHd"
      },
      "outputs": [],
      "source": [
        "def rmse(y_true,y_pred):\n",
        "    return np.sqrt(mse(y_true,y_pred))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "enLmpBgaKsHe"
      },
      "source": [
        "## 4a. Prediction using Lasso regression"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "4_V70OgCKsHi",
        "outputId": "b432480a-3b09-4af1-9c38-373a9f783b24",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 100/100 [00:02<00:00, 37.44it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "RMSE for lasso: 0.21830691185854453, alpha = 0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "# Lasso regression\n",
        "best_lasso_model = None\n",
        "best_lasso_score = float('inf')\n",
        "best_alpha = 0\n",
        "for a in tqdm(range(0,100)):\n",
        "    lasso_model = Lasso(alpha=a/10).fit(x_train, y_train)\n",
        "    val_pred_lasso = lasso_model.predict(x_val)\n",
        "    val_score_lasso = rmse(val_pred_lasso,y_val)\n",
        "    if val_score_lasso < best_lasso_score:\n",
        "        best_alpha = a\n",
        "        best_lasso_score = val_score_lasso\n",
        "        best_lasso_model = lasso_model\n",
        "print(f'RMSE for lasso: {best_lasso_score}, alpha = {best_alpha}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uUwTBOfBKsHq"
      },
      "source": [
        "## 4b. Prediction using Ridge regression"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "CTMDEHA4KsHs",
        "outputId": "91a90a5f-5e22-4624-ec83-f8fc0df6cdc3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 100/100 [00:01<00:00, 78.11it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "RMSE for ridge: 0.21830698469763316, alpha = 1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "# Ridge regression\n",
        "best_ridge_model = None\n",
        "best_ridge_score = float('inf')\n",
        "best_alpha = 1\n",
        "for a in tqdm(range(0,100)):\n",
        "    ridge_model = Ridge(alpha=a/10).fit(x_train,y_train)\n",
        "    val_pred_ridge = ridge_model.predict(x_val)\n",
        "    val_score_ridge = rmse(val_pred_ridge,y_val)\n",
        "    if val_score_ridge < best_ridge_score:\n",
        "        best_alpha = a\n",
        "        best_ridge_score = val_score_ridge\n",
        "        best_ridge_model = ridge_model\n",
        "print(f'RMSE for ridge: {best_ridge_score}, alpha = {best_alpha}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y93VaZX9KsHv"
      },
      "source": [
        "## 4c. Prediction using Random Forests"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "xAHEcXYTKsHx",
        "outputId": "7cef2ec1-2818-45a3-c25d-dc8ee4b267d8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "RMSE for Random Forest: 0.12318053048520272\n"
          ]
        }
      ],
      "source": [
        "# RandomForest \n",
        "rf_model = RandomForestRegressor(n_estimators = 500,\n",
        "                                min_samples_split = 2,\n",
        "                                min_samples_leaf = 1,\n",
        "                                max_samples = 0.8\n",
        "                                )\n",
        "rf_model.fit(x_train,y_train)\n",
        "val_pred_rf = rf_model.predict(x_val)\n",
        "val_score_rf = rmse(val_pred_rf,y_val)\n",
        "print(f'RMSE for Random Forest: {val_score_rf}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9j6uzkzkKsH0"
      },
      "source": [
        "## 4d. Prediction using LGBM"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "zR81FO8ZKsH1",
        "outputId": "8c0a77b6-62ac-406a-e963-78f4a7beefcc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training until validation scores don't improve for 30 rounds.\n",
            "[100]\ttraining's rmse: 0.104317\tvalid_1's rmse: 0.118665\n",
            "[200]\ttraining's rmse: 0.0981742\tvalid_1's rmse: 0.117294\n",
            "Early stopping, best iteration is:\n",
            "[209]\ttraining's rmse: 0.0979237\tvalid_1's rmse: 0.117245\n"
          ]
        }
      ],
      "source": [
        "# LGBM\n",
        "lgb_params = {\n",
        "        'boosting_type': 'gbdt',         \n",
        "        'objective': 'regression',       \n",
        "        'metric': ['rmse'],             \n",
        "        'subsample': 0.5,                \n",
        "        'subsample_freq': 1,\n",
        "        'learning_rate': 0.05,           \n",
        "        'num_leaves': 2**8,            \n",
        "        'min_data_in_leaf': 2**4,      \n",
        "        'feature_fraction': 0.5,\n",
        "        'n_estimators': 5000,            \n",
        "        'early_stopping_rounds': 30,     \n",
        "        'verbose': -1,\n",
        "            } \n",
        "train_set = lgb.Dataset(x_train, y_train)\n",
        "val_set = lgb.Dataset(x_val, y_val)\n",
        "lgb_model = lgb.train(lgb_params, train_set, num_boost_round = 2000, valid_sets = [train_set, val_set], verbose_eval = 100)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mQkp45B2KsH5"
      },
      "source": [
        "## 4e. Prediction using deep neural network"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "hBfxaYjXKsH8"
      },
      "outputs": [],
      "source": [
        "# training configurations\n",
        "from keras import backend as K\n",
        "\n",
        "def keras_rmse(y_true, y_pred):\n",
        "        return K.sqrt(K.mean(K.square(y_pred - y_true))) \n",
        "    \n",
        "num_epochs = 100\n",
        "batch_size = 4\n",
        "num_nodes = 256\n",
        "num_layers = 3\n",
        "dropout = 0.2\n",
        "loss_fn = keras_rmse\n",
        "optimizer = 'adagrad'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "vfPrX13mKsH9",
        "outputId": "061de6ed-ac12-435e-a8e9-769a2d8b42c6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100\n",
            "3283/3293 [============================>.] - ETA: 0s - loss: 0.1478 - accuracy: 0.8321\n",
            "Epoch 1: val_accuracy improved from -inf to 0.83182, saving model to mlp_best_model.h5\n",
            "3293/3293 [==============================] - 10s 3ms/step - loss: 0.1477 - accuracy: 0.8321 - val_loss: 0.1390 - val_accuracy: 0.8318\n",
            "Epoch 2/100\n",
            "3280/3293 [============================>.] - ETA: 0s - loss: 0.1419 - accuracy: 0.8318\n",
            "Epoch 2: val_accuracy did not improve from 0.83182\n",
            "3293/3293 [==============================] - 8s 2ms/step - loss: 0.1414 - accuracy: 0.8321 - val_loss: 0.1383 - val_accuracy: 0.8318\n",
            "Epoch 3/100\n",
            "3286/3293 [============================>.] - ETA: 0s - loss: 0.1406 - accuracy: 0.8322\n",
            "Epoch 3: val_accuracy did not improve from 0.83182\n",
            "3293/3293 [==============================] - 9s 3ms/step - loss: 0.1405 - accuracy: 0.8321 - val_loss: 0.1381 - val_accuracy: 0.8318\n",
            "Epoch 4/100\n",
            "3288/3293 [============================>.] - ETA: 0s - loss: 0.1384 - accuracy: 0.8320\n",
            "Epoch 4: val_accuracy did not improve from 0.83182\n",
            "3293/3293 [==============================] - 8s 3ms/step - loss: 0.1384 - accuracy: 0.8321 - val_loss: 0.1379 - val_accuracy: 0.8318\n",
            "Epoch 5/100\n",
            "3272/3293 [============================>.] - ETA: 0s - loss: 0.1397 - accuracy: 0.8324\n",
            "Epoch 5: val_accuracy did not improve from 0.83182\n",
            "3293/3293 [==============================] - 8s 3ms/step - loss: 0.1403 - accuracy: 0.8321 - val_loss: 0.1379 - val_accuracy: 0.8318\n",
            "Epoch 6/100\n",
            "3273/3293 [============================>.] - ETA: 0s - loss: 0.1408 - accuracy: 0.8320\n",
            "Epoch 6: val_accuracy did not improve from 0.83182\n",
            "3293/3293 [==============================] - 8s 3ms/step - loss: 0.1404 - accuracy: 0.8321 - val_loss: 0.1378 - val_accuracy: 0.8318\n",
            "Epoch 7/100\n",
            "3287/3293 [============================>.] - ETA: 0s - loss: 0.1397 - accuracy: 0.8324\n",
            "Epoch 7: val_accuracy did not improve from 0.83182\n",
            "3293/3293 [==============================] - 9s 3ms/step - loss: 0.1401 - accuracy: 0.8321 - val_loss: 0.1378 - val_accuracy: 0.8318\n",
            "Epoch 8/100\n",
            "3284/3293 [============================>.] - ETA: 0s - loss: 0.1393 - accuracy: 0.8320\n",
            "Epoch 8: val_accuracy did not improve from 0.83182\n",
            "3293/3293 [==============================] - 8s 3ms/step - loss: 0.1392 - accuracy: 0.8321 - val_loss: 0.1378 - val_accuracy: 0.8318\n",
            "Epoch 9/100\n",
            "3292/3293 [============================>.] - ETA: 0s - loss: 0.1403 - accuracy: 0.8322\n",
            "Epoch 9: val_accuracy did not improve from 0.83182\n",
            "3293/3293 [==============================] - 8s 3ms/step - loss: 0.1403 - accuracy: 0.8321 - val_loss: 0.1377 - val_accuracy: 0.8318\n",
            "Epoch 10/100\n",
            "3277/3293 [============================>.] - ETA: 0s - loss: 0.1385 - accuracy: 0.8325\n",
            "Epoch 10: val_accuracy did not improve from 0.83182\n",
            "3293/3293 [==============================] - 8s 3ms/step - loss: 0.1389 - accuracy: 0.8321 - val_loss: 0.1377 - val_accuracy: 0.8318\n",
            "Epoch 11/100\n",
            "3280/3293 [============================>.] - ETA: 0s - loss: 0.1399 - accuracy: 0.8321\n",
            "Epoch 11: val_accuracy did not improve from 0.83182\n",
            "3293/3293 [==============================] - 8s 2ms/step - loss: 0.1398 - accuracy: 0.8321 - val_loss: 0.1377 - val_accuracy: 0.8318\n",
            "Epoch 12/100\n",
            "3293/3293 [==============================] - ETA: 0s - loss: 0.1403 - accuracy: 0.8321\n",
            "Epoch 12: val_accuracy did not improve from 0.83182\n",
            "3293/3293 [==============================] - 8s 2ms/step - loss: 0.1403 - accuracy: 0.8321 - val_loss: 0.1377 - val_accuracy: 0.8318\n",
            "Epoch 13/100\n",
            "3292/3293 [============================>.] - ETA: 0s - loss: 0.1405 - accuracy: 0.8322\n",
            "Epoch 13: val_accuracy did not improve from 0.83182\n",
            "3293/3293 [==============================] - 8s 2ms/step - loss: 0.1406 - accuracy: 0.8321 - val_loss: 0.1377 - val_accuracy: 0.8318\n",
            "Epoch 14/100\n",
            "3291/3293 [============================>.] - ETA: 0s - loss: 0.1389 - accuracy: 0.8323\n",
            "Epoch 14: val_accuracy did not improve from 0.83182\n",
            "3293/3293 [==============================] - 8s 2ms/step - loss: 0.1390 - accuracy: 0.8321 - val_loss: 0.1377 - val_accuracy: 0.8318\n",
            "Epoch 15/100\n",
            "3289/3293 [============================>.] - ETA: 0s - loss: 0.1405 - accuracy: 0.8322\n",
            "Epoch 15: val_accuracy did not improve from 0.83182\n",
            "3293/3293 [==============================] - 8s 3ms/step - loss: 0.1406 - accuracy: 0.8321 - val_loss: 0.1377 - val_accuracy: 0.8318\n",
            "Epoch 16/100\n",
            "3285/3293 [============================>.] - ETA: 0s - loss: 0.1391 - accuracy: 0.8320\n",
            "Epoch 16: val_accuracy did not improve from 0.83182\n",
            "3293/3293 [==============================] - 8s 2ms/step - loss: 0.1391 - accuracy: 0.8321 - val_loss: 0.1377 - val_accuracy: 0.8318\n",
            "Epoch 17/100\n",
            "3273/3293 [============================>.] - ETA: 0s - loss: 0.1386 - accuracy: 0.8325\n",
            "Epoch 17: val_accuracy did not improve from 0.83182\n",
            "3293/3293 [==============================] - 8s 3ms/step - loss: 0.1389 - accuracy: 0.8321 - val_loss: 0.1376 - val_accuracy: 0.8318\n",
            "Epoch 18/100\n",
            "3278/3293 [============================>.] - ETA: 0s - loss: 0.1396 - accuracy: 0.8324\n",
            "Epoch 18: val_accuracy did not improve from 0.83182\n",
            "3293/3293 [==============================] - 8s 3ms/step - loss: 0.1395 - accuracy: 0.8321 - val_loss: 0.1376 - val_accuracy: 0.8318\n",
            "Epoch 19/100\n",
            "3289/3293 [============================>.] - ETA: 0s - loss: 0.1389 - accuracy: 0.8319\n",
            "Epoch 19: val_accuracy did not improve from 0.83182\n",
            "3293/3293 [==============================] - 8s 2ms/step - loss: 0.1387 - accuracy: 0.8321 - val_loss: 0.1376 - val_accuracy: 0.8318\n",
            "Epoch 20/100\n",
            "3291/3293 [============================>.] - ETA: 0s - loss: 0.1397 - accuracy: 0.8323\n",
            "Epoch 20: val_accuracy did not improve from 0.83182\n",
            "3293/3293 [==============================] - 8s 2ms/step - loss: 0.1396 - accuracy: 0.8321 - val_loss: 0.1376 - val_accuracy: 0.8318\n",
            "Epoch 21/100\n",
            "3272/3293 [============================>.] - ETA: 0s - loss: 0.1397 - accuracy: 0.8323\n",
            "Epoch 21: val_accuracy did not improve from 0.83182\n",
            "3293/3293 [==============================] - 8s 2ms/step - loss: 0.1401 - accuracy: 0.8321 - val_loss: 0.1376 - val_accuracy: 0.8318\n",
            "Epoch 22/100\n",
            "3291/3293 [============================>.] - ETA: 0s - loss: 0.1396 - accuracy: 0.8321\n",
            "Epoch 22: val_accuracy did not improve from 0.83182\n",
            "3293/3293 [==============================] - 8s 2ms/step - loss: 0.1395 - accuracy: 0.8321 - val_loss: 0.1376 - val_accuracy: 0.8318\n",
            "Epoch 23/100\n",
            "3274/3293 [============================>.] - ETA: 0s - loss: 0.1390 - accuracy: 0.8319\n",
            "Epoch 23: val_accuracy did not improve from 0.83182\n",
            "3293/3293 [==============================] - 8s 2ms/step - loss: 0.1390 - accuracy: 0.8321 - val_loss: 0.1376 - val_accuracy: 0.8318\n",
            "Epoch 24/100\n",
            "3291/3293 [============================>.] - ETA: 0s - loss: 0.1384 - accuracy: 0.8320\n",
            "Epoch 24: val_accuracy did not improve from 0.83182\n",
            "3293/3293 [==============================] - 8s 2ms/step - loss: 0.1383 - accuracy: 0.8321 - val_loss: 0.1376 - val_accuracy: 0.8318\n",
            "Epoch 25/100\n",
            "3287/3293 [============================>.] - ETA: 0s - loss: 0.1375 - accuracy: 0.8322\n",
            "Epoch 25: val_accuracy did not improve from 0.83182\n",
            "3293/3293 [==============================] - 8s 2ms/step - loss: 0.1376 - accuracy: 0.8321 - val_loss: 0.1376 - val_accuracy: 0.8318\n",
            "Epoch 26/100\n",
            "3282/3293 [============================>.] - ETA: 0s - loss: 0.1384 - accuracy: 0.8322\n",
            "Epoch 26: val_accuracy did not improve from 0.83182\n",
            "3293/3293 [==============================] - 8s 3ms/step - loss: 0.1386 - accuracy: 0.8321 - val_loss: 0.1376 - val_accuracy: 0.8318\n",
            "Epoch 27/100\n",
            "3283/3293 [============================>.] - ETA: 0s - loss: 0.1396 - accuracy: 0.8322\n",
            "Epoch 27: val_accuracy did not improve from 0.83182\n",
            "3293/3293 [==============================] - 8s 3ms/step - loss: 0.1395 - accuracy: 0.8321 - val_loss: 0.1376 - val_accuracy: 0.8318\n",
            "Epoch 28/100\n",
            "3288/3293 [============================>.] - ETA: 0s - loss: 0.1390 - accuracy: 0.8322\n",
            "Epoch 28: val_accuracy did not improve from 0.83182\n",
            "3293/3293 [==============================] - 8s 2ms/step - loss: 0.1392 - accuracy: 0.8321 - val_loss: 0.1376 - val_accuracy: 0.8318\n",
            "Epoch 29/100\n",
            "3281/3293 [============================>.] - ETA: 0s - loss: 0.1396 - accuracy: 0.8320\n",
            "Epoch 29: val_accuracy did not improve from 0.83182\n",
            "3293/3293 [==============================] - 8s 2ms/step - loss: 0.1395 - accuracy: 0.8321 - val_loss: 0.1376 - val_accuracy: 0.8318\n",
            "Epoch 30/100\n",
            "3286/3293 [============================>.] - ETA: 0s - loss: 0.1390 - accuracy: 0.8320\n",
            "Epoch 30: val_accuracy did not improve from 0.83182\n",
            "3293/3293 [==============================] - 9s 3ms/step - loss: 0.1388 - accuracy: 0.8321 - val_loss: 0.1376 - val_accuracy: 0.8318\n",
            "Epoch 31/100\n",
            "3286/3293 [============================>.] - ETA: 0s - loss: 0.1405 - accuracy: 0.8319\n",
            "Epoch 31: val_accuracy did not improve from 0.83182\n",
            "3293/3293 [==============================] - 9s 3ms/step - loss: 0.1402 - accuracy: 0.8321 - val_loss: 0.1376 - val_accuracy: 0.8318\n",
            "Epoch 32/100\n",
            "3277/3293 [============================>.] - ETA: 0s - loss: 0.1395 - accuracy: 0.8323\n",
            "Epoch 32: val_accuracy did not improve from 0.83182\n",
            "3293/3293 [==============================] - 8s 2ms/step - loss: 0.1397 - accuracy: 0.8321 - val_loss: 0.1376 - val_accuracy: 0.8318\n",
            "Epoch 33/100\n",
            "3282/3293 [============================>.] - ETA: 0s - loss: 0.1405 - accuracy: 0.8320\n",
            "Epoch 33: val_accuracy did not improve from 0.83182\n",
            "3293/3293 [==============================] - 8s 3ms/step - loss: 0.1401 - accuracy: 0.8321 - val_loss: 0.1376 - val_accuracy: 0.8318\n",
            "Epoch 34/100\n",
            "3293/3293 [==============================] - ETA: 0s - loss: 0.1390 - accuracy: 0.8321\n",
            "Epoch 34: val_accuracy did not improve from 0.83182\n",
            "3293/3293 [==============================] - 9s 3ms/step - loss: 0.1390 - accuracy: 0.8321 - val_loss: 0.1376 - val_accuracy: 0.8318\n",
            "Epoch 35/100\n",
            "3275/3293 [============================>.] - ETA: 0s - loss: 0.1398 - accuracy: 0.8318\n",
            "Epoch 35: val_accuracy did not improve from 0.83182\n",
            "3293/3293 [==============================] - 8s 2ms/step - loss: 0.1396 - accuracy: 0.8321 - val_loss: 0.1376 - val_accuracy: 0.8318\n",
            "Epoch 36/100\n",
            "3281/3293 [============================>.] - ETA: 0s - loss: 0.1390 - accuracy: 0.8324\n",
            "Epoch 36: val_accuracy did not improve from 0.83182\n",
            "3293/3293 [==============================] - 8s 3ms/step - loss: 0.1393 - accuracy: 0.8321 - val_loss: 0.1376 - val_accuracy: 0.8318\n",
            "Epoch 37/100\n",
            "3286/3293 [============================>.] - ETA: 0s - loss: 0.1392 - accuracy: 0.8320\n",
            "Epoch 37: val_accuracy did not improve from 0.83182\n",
            "3293/3293 [==============================] - 8s 2ms/step - loss: 0.1390 - accuracy: 0.8321 - val_loss: 0.1376 - val_accuracy: 0.8318\n",
            "Epoch 38/100\n",
            "3270/3293 [============================>.] - ETA: 0s - loss: 0.1399 - accuracy: 0.8323\n",
            "Epoch 38: val_accuracy did not improve from 0.83182\n",
            "3293/3293 [==============================] - 8s 2ms/step - loss: 0.1401 - accuracy: 0.8321 - val_loss: 0.1376 - val_accuracy: 0.8318\n",
            "Epoch 39/100\n",
            "3270/3293 [============================>.] - ETA: 0s - loss: 0.1388 - accuracy: 0.8325\n",
            "Epoch 39: val_accuracy did not improve from 0.83182\n",
            "3293/3293 [==============================] - 9s 3ms/step - loss: 0.1392 - accuracy: 0.8321 - val_loss: 0.1376 - val_accuracy: 0.8318\n",
            "Epoch 40/100\n",
            "3291/3293 [============================>.] - ETA: 0s - loss: 0.1400 - accuracy: 0.8321\n",
            "Epoch 40: val_accuracy did not improve from 0.83182\n",
            "3293/3293 [==============================] - 9s 3ms/step - loss: 0.1400 - accuracy: 0.8321 - val_loss: 0.1376 - val_accuracy: 0.8318\n",
            "Epoch 41/100\n",
            "3284/3293 [============================>.] - ETA: 0s - loss: 0.1392 - accuracy: 0.8324\n",
            "Epoch 41: val_accuracy did not improve from 0.83182\n",
            "3293/3293 [==============================] - 9s 3ms/step - loss: 0.1395 - accuracy: 0.8321 - val_loss: 0.1376 - val_accuracy: 0.8318\n",
            "Epoch 42/100\n",
            "3276/3293 [============================>.] - ETA: 0s - loss: 0.1396 - accuracy: 0.8320\n",
            "Epoch 42: val_accuracy did not improve from 0.83182\n",
            "3293/3293 [==============================] - 8s 3ms/step - loss: 0.1392 - accuracy: 0.8321 - val_loss: 0.1376 - val_accuracy: 0.8318\n",
            "Epoch 43/100\n",
            "3291/3293 [============================>.] - ETA: 0s - loss: 0.1402 - accuracy: 0.8321\n",
            "Epoch 43: val_accuracy did not improve from 0.83182\n",
            "3293/3293 [==============================] - 8s 3ms/step - loss: 0.1401 - accuracy: 0.8321 - val_loss: 0.1376 - val_accuracy: 0.8318\n",
            "Epoch 44/100\n",
            "3284/3293 [============================>.] - ETA: 0s - loss: 0.1386 - accuracy: 0.8321\n",
            "Epoch 44: val_accuracy did not improve from 0.83182\n",
            "3293/3293 [==============================] - 8s 3ms/step - loss: 0.1387 - accuracy: 0.8321 - val_loss: 0.1376 - val_accuracy: 0.8318\n",
            "Epoch 45/100\n",
            "3280/3293 [============================>.] - ETA: 0s - loss: 0.1395 - accuracy: 0.8319\n",
            "Epoch 45: val_accuracy did not improve from 0.83182\n",
            "3293/3293 [==============================] - 9s 3ms/step - loss: 0.1394 - accuracy: 0.8321 - val_loss: 0.1376 - val_accuracy: 0.8318\n",
            "Epoch 46/100\n",
            "3293/3293 [==============================] - ETA: 0s - loss: 0.1401 - accuracy: 0.8321\n",
            "Epoch 46: val_accuracy did not improve from 0.83182\n",
            "3293/3293 [==============================] - 9s 3ms/step - loss: 0.1401 - accuracy: 0.8321 - val_loss: 0.1376 - val_accuracy: 0.8318\n",
            "Epoch 47/100\n",
            "3292/3293 [============================>.] - ETA: 0s - loss: 0.1394 - accuracy: 0.8322\n",
            "Epoch 47: val_accuracy did not improve from 0.83182\n",
            "3293/3293 [==============================] - 9s 3ms/step - loss: 0.1394 - accuracy: 0.8321 - val_loss: 0.1376 - val_accuracy: 0.8318\n",
            "Epoch 48/100\n",
            "3291/3293 [============================>.] - ETA: 0s - loss: 0.1394 - accuracy: 0.8320\n",
            "Epoch 48: val_accuracy did not improve from 0.83182\n",
            "3293/3293 [==============================] - 9s 3ms/step - loss: 0.1394 - accuracy: 0.8321 - val_loss: 0.1376 - val_accuracy: 0.8318\n",
            "Epoch 49/100\n",
            "3291/3293 [============================>.] - ETA: 0s - loss: 0.1382 - accuracy: 0.8320\n",
            "Epoch 49: val_accuracy did not improve from 0.83182\n",
            "3293/3293 [==============================] - 8s 2ms/step - loss: 0.1381 - accuracy: 0.8321 - val_loss: 0.1376 - val_accuracy: 0.8318\n",
            "Epoch 50/100\n",
            "3271/3293 [============================>.] - ETA: 0s - loss: 0.1372 - accuracy: 0.8323\n",
            "Epoch 50: val_accuracy did not improve from 0.83182\n",
            "3293/3293 [==============================] - 9s 3ms/step - loss: 0.1375 - accuracy: 0.8321 - val_loss: 0.1376 - val_accuracy: 0.8318\n",
            "Epoch 51/100\n",
            "3274/3293 [============================>.] - ETA: 0s - loss: 0.1395 - accuracy: 0.8321\n",
            "Epoch 51: val_accuracy did not improve from 0.83182\n",
            "3293/3293 [==============================] - 9s 3ms/step - loss: 0.1394 - accuracy: 0.8321 - val_loss: 0.1376 - val_accuracy: 0.8318\n",
            "Epoch 52/100\n",
            "3285/3293 [============================>.] - ETA: 0s - loss: 0.1393 - accuracy: 0.8323\n",
            "Epoch 52: val_accuracy did not improve from 0.83182\n",
            "3293/3293 [==============================] - 9s 3ms/step - loss: 0.1395 - accuracy: 0.8321 - val_loss: 0.1376 - val_accuracy: 0.8318\n",
            "Epoch 53/100\n",
            "3291/3293 [============================>.] - ETA: 0s - loss: 0.1395 - accuracy: 0.8322\n",
            "Epoch 53: val_accuracy did not improve from 0.83182\n",
            "3293/3293 [==============================] - 8s 2ms/step - loss: 0.1398 - accuracy: 0.8321 - val_loss: 0.1376 - val_accuracy: 0.8318\n",
            "Epoch 54/100\n",
            "3277/3293 [============================>.] - ETA: 0s - loss: 0.1399 - accuracy: 0.8322\n",
            "Epoch 54: val_accuracy did not improve from 0.83182\n",
            "3293/3293 [==============================] - 9s 3ms/step - loss: 0.1400 - accuracy: 0.8321 - val_loss: 0.1376 - val_accuracy: 0.8318\n",
            "Epoch 55/100\n",
            "3281/3293 [============================>.] - ETA: 0s - loss: 0.1402 - accuracy: 0.8319\n",
            "Epoch 55: val_accuracy did not improve from 0.83182\n",
            "3293/3293 [==============================] - 9s 3ms/step - loss: 0.1400 - accuracy: 0.8321 - val_loss: 0.1376 - val_accuracy: 0.8318\n",
            "Epoch 56/100\n",
            "3285/3293 [============================>.] - ETA: 0s - loss: 0.1395 - accuracy: 0.8323\n",
            "Epoch 56: val_accuracy did not improve from 0.83182\n",
            "3293/3293 [==============================] - 9s 3ms/step - loss: 0.1396 - accuracy: 0.8321 - val_loss: 0.1376 - val_accuracy: 0.8318\n",
            "Epoch 57/100\n",
            "3281/3293 [============================>.] - ETA: 0s - loss: 0.1395 - accuracy: 0.8324\n",
            "Epoch 57: val_accuracy did not improve from 0.83182\n",
            "3293/3293 [==============================] - 8s 2ms/step - loss: 0.1394 - accuracy: 0.8321 - val_loss: 0.1376 - val_accuracy: 0.8318\n",
            "Epoch 58/100\n",
            "3280/3293 [============================>.] - ETA: 0s - loss: 0.1390 - accuracy: 0.8318\n",
            "Epoch 58: val_accuracy did not improve from 0.83182\n",
            "3293/3293 [==============================] - 9s 3ms/step - loss: 0.1387 - accuracy: 0.8321 - val_loss: 0.1376 - val_accuracy: 0.8318\n",
            "Epoch 59/100\n",
            "3291/3293 [============================>.] - ETA: 0s - loss: 0.1402 - accuracy: 0.8323\n",
            "Epoch 59: val_accuracy did not improve from 0.83182\n",
            "3293/3293 [==============================] - 9s 3ms/step - loss: 0.1402 - accuracy: 0.8321 - val_loss: 0.1376 - val_accuracy: 0.8318\n",
            "Epoch 60/100\n",
            "3285/3293 [============================>.] - ETA: 0s - loss: 0.1398 - accuracy: 0.8325\n",
            "Epoch 60: val_accuracy did not improve from 0.83182\n",
            "3293/3293 [==============================] - 9s 3ms/step - loss: 0.1398 - accuracy: 0.8321 - val_loss: 0.1376 - val_accuracy: 0.8318\n",
            "Epoch 61/100\n",
            "3276/3293 [============================>.] - ETA: 0s - loss: 0.1393 - accuracy: 0.8324\n",
            "Epoch 61: val_accuracy did not improve from 0.83182\n",
            "3293/3293 [==============================] - 8s 2ms/step - loss: 0.1393 - accuracy: 0.8321 - val_loss: 0.1376 - val_accuracy: 0.8318\n",
            "Epoch 62/100\n",
            "3288/3293 [============================>.] - ETA: 0s - loss: 0.1398 - accuracy: 0.8320\n",
            "Epoch 62: val_accuracy did not improve from 0.83182\n",
            "3293/3293 [==============================] - 8s 2ms/step - loss: 0.1397 - accuracy: 0.8321 - val_loss: 0.1376 - val_accuracy: 0.8318\n",
            "Epoch 63/100\n",
            "3272/3293 [============================>.] - ETA: 0s - loss: 0.1397 - accuracy: 0.8323\n",
            "Epoch 63: val_accuracy did not improve from 0.83182\n",
            "3293/3293 [==============================] - 8s 2ms/step - loss: 0.1397 - accuracy: 0.8321 - val_loss: 0.1376 - val_accuracy: 0.8318\n",
            "Epoch 64/100\n",
            "3285/3293 [============================>.] - ETA: 0s - loss: 0.1397 - accuracy: 0.8323\n",
            "Epoch 64: val_accuracy did not improve from 0.83182\n",
            "3293/3293 [==============================] - 8s 2ms/step - loss: 0.1399 - accuracy: 0.8321 - val_loss: 0.1376 - val_accuracy: 0.8318\n",
            "Epoch 65/100\n",
            "3281/3293 [============================>.] - ETA: 0s - loss: 0.1376 - accuracy: 0.8322\n",
            "Epoch 65: val_accuracy did not improve from 0.83182\n",
            "3293/3293 [==============================] - 8s 2ms/step - loss: 0.1375 - accuracy: 0.8321 - val_loss: 0.1376 - val_accuracy: 0.8318\n",
            "Epoch 66/100\n",
            "3292/3293 [============================>.] - ETA: 0s - loss: 0.1395 - accuracy: 0.8321\n",
            "Epoch 66: val_accuracy did not improve from 0.83182\n",
            "3293/3293 [==============================] - 8s 3ms/step - loss: 0.1395 - accuracy: 0.8321 - val_loss: 0.1376 - val_accuracy: 0.8318\n",
            "Epoch 67/100\n",
            "3283/3293 [============================>.] - ETA: 0s - loss: 0.1393 - accuracy: 0.8321\n",
            "Epoch 67: val_accuracy did not improve from 0.83182\n",
            "3293/3293 [==============================] - 8s 3ms/step - loss: 0.1392 - accuracy: 0.8321 - val_loss: 0.1376 - val_accuracy: 0.8318\n",
            "Epoch 68/100\n",
            "3290/3293 [============================>.] - ETA: 0s - loss: 0.1408 - accuracy: 0.8320\n",
            "Epoch 68: val_accuracy did not improve from 0.83182\n",
            "3293/3293 [==============================] - 8s 3ms/step - loss: 0.1406 - accuracy: 0.8321 - val_loss: 0.1376 - val_accuracy: 0.8318\n",
            "Epoch 69/100\n",
            "3291/3293 [============================>.] - ETA: 0s - loss: 0.1385 - accuracy: 0.8321\n",
            "Epoch 69: val_accuracy did not improve from 0.83182\n",
            "3293/3293 [==============================] - 8s 2ms/step - loss: 0.1385 - accuracy: 0.8321 - val_loss: 0.1376 - val_accuracy: 0.8318\n",
            "Epoch 70/100\n",
            "3293/3293 [==============================] - ETA: 0s - loss: 0.1400 - accuracy: 0.8321\n",
            "Epoch 70: val_accuracy did not improve from 0.83182\n",
            "3293/3293 [==============================] - 8s 3ms/step - loss: 0.1400 - accuracy: 0.8321 - val_loss: 0.1376 - val_accuracy: 0.8318\n",
            "Epoch 71/100\n",
            "3277/3293 [============================>.] - ETA: 0s - loss: 0.1393 - accuracy: 0.8320\n",
            "Epoch 71: val_accuracy did not improve from 0.83182\n",
            "3293/3293 [==============================] - 8s 3ms/step - loss: 0.1394 - accuracy: 0.8321 - val_loss: 0.1376 - val_accuracy: 0.8318\n",
            "Epoch 72/100\n",
            "3292/3293 [============================>.] - ETA: 0s - loss: 0.1409 - accuracy: 0.8321\n",
            "Epoch 72: val_accuracy did not improve from 0.83182\n",
            "3293/3293 [==============================] - 8s 2ms/step - loss: 0.1409 - accuracy: 0.8321 - val_loss: 0.1376 - val_accuracy: 0.8318\n",
            "Epoch 73/100\n",
            "3284/3293 [============================>.] - ETA: 0s - loss: 0.1384 - accuracy: 0.8321\n",
            "Epoch 73: val_accuracy did not improve from 0.83182\n",
            "3293/3293 [==============================] - 8s 2ms/step - loss: 0.1382 - accuracy: 0.8321 - val_loss: 0.1376 - val_accuracy: 0.8318\n",
            "Epoch 74/100\n",
            "3278/3293 [============================>.] - ETA: 0s - loss: 0.1390 - accuracy: 0.8319\n",
            "Epoch 74: val_accuracy did not improve from 0.83182\n",
            "3293/3293 [==============================] - 9s 3ms/step - loss: 0.1391 - accuracy: 0.8321 - val_loss: 0.1376 - val_accuracy: 0.8318\n",
            "Epoch 75/100\n",
            "3276/3293 [============================>.] - ETA: 0s - loss: 0.1399 - accuracy: 0.8327\n",
            "Epoch 75: val_accuracy did not improve from 0.83182\n",
            "3293/3293 [==============================] - 8s 2ms/step - loss: 0.1402 - accuracy: 0.8321 - val_loss: 0.1376 - val_accuracy: 0.8318\n",
            "Epoch 76/100\n",
            "3283/3293 [============================>.] - ETA: 0s - loss: 0.1401 - accuracy: 0.8322\n",
            "Epoch 76: val_accuracy did not improve from 0.83182\n",
            "3293/3293 [==============================] - 9s 3ms/step - loss: 0.1401 - accuracy: 0.8321 - val_loss: 0.1376 - val_accuracy: 0.8318\n",
            "Epoch 77/100\n",
            "3290/3293 [============================>.] - ETA: 0s - loss: 0.1396 - accuracy: 0.8322\n",
            "Epoch 77: val_accuracy did not improve from 0.83182\n",
            "3293/3293 [==============================] - 9s 3ms/step - loss: 0.1396 - accuracy: 0.8321 - val_loss: 0.1376 - val_accuracy: 0.8318\n",
            "Epoch 78/100\n",
            "3273/3293 [============================>.] - ETA: 0s - loss: 0.1404 - accuracy: 0.8320\n",
            "Epoch 78: val_accuracy did not improve from 0.83182\n",
            "3293/3293 [==============================] - 8s 3ms/step - loss: 0.1404 - accuracy: 0.8321 - val_loss: 0.1376 - val_accuracy: 0.8318\n",
            "Epoch 79/100\n",
            "3287/3293 [============================>.] - ETA: 0s - loss: 0.1394 - accuracy: 0.8319\n",
            "Epoch 79: val_accuracy did not improve from 0.83182\n",
            "3293/3293 [==============================] - 8s 3ms/step - loss: 0.1391 - accuracy: 0.8321 - val_loss: 0.1376 - val_accuracy: 0.8318\n",
            "Epoch 80/100\n",
            "3290/3293 [============================>.] - ETA: 0s - loss: 0.1405 - accuracy: 0.8321\n",
            "Epoch 80: val_accuracy did not improve from 0.83182\n",
            "3293/3293 [==============================] - 8s 3ms/step - loss: 0.1405 - accuracy: 0.8321 - val_loss: 0.1376 - val_accuracy: 0.8318\n",
            "Epoch 81/100\n",
            "3282/3293 [============================>.] - ETA: 0s - loss: 0.1391 - accuracy: 0.8323\n",
            "Epoch 81: val_accuracy did not improve from 0.83182\n",
            "3293/3293 [==============================] - 9s 3ms/step - loss: 0.1393 - accuracy: 0.8321 - val_loss: 0.1376 - val_accuracy: 0.8318\n",
            "Epoch 82/100\n",
            "3292/3293 [============================>.] - ETA: 0s - loss: 0.1402 - accuracy: 0.8322\n",
            "Epoch 82: val_accuracy did not improve from 0.83182\n",
            "3293/3293 [==============================] - 8s 2ms/step - loss: 0.1403 - accuracy: 0.8321 - val_loss: 0.1376 - val_accuracy: 0.8318\n",
            "Epoch 83/100\n",
            "3282/3293 [============================>.] - ETA: 0s - loss: 0.1386 - accuracy: 0.8320\n",
            "Epoch 83: val_accuracy did not improve from 0.83182\n",
            "3293/3293 [==============================] - 9s 3ms/step - loss: 0.1386 - accuracy: 0.8321 - val_loss: 0.1376 - val_accuracy: 0.8318\n",
            "Epoch 84/100\n",
            "3270/3293 [============================>.] - ETA: 0s - loss: 0.1398 - accuracy: 0.8325\n",
            "Epoch 84: val_accuracy did not improve from 0.83182\n",
            "3293/3293 [==============================] - 8s 2ms/step - loss: 0.1398 - accuracy: 0.8321 - val_loss: 0.1376 - val_accuracy: 0.8318\n",
            "Epoch 85/100\n",
            "3277/3293 [============================>.] - ETA: 0s - loss: 0.1394 - accuracy: 0.8320\n",
            "Epoch 85: val_accuracy did not improve from 0.83182\n",
            "3293/3293 [==============================] - 9s 3ms/step - loss: 0.1393 - accuracy: 0.8321 - val_loss: 0.1376 - val_accuracy: 0.8318\n",
            "Epoch 86/100\n",
            "3275/3293 [============================>.] - ETA: 0s - loss: 0.1406 - accuracy: 0.8321\n",
            "Epoch 86: val_accuracy did not improve from 0.83182\n",
            "3293/3293 [==============================] - 9s 3ms/step - loss: 0.1406 - accuracy: 0.8321 - val_loss: 0.1376 - val_accuracy: 0.8318\n",
            "Epoch 87/100\n",
            "3282/3293 [============================>.] - ETA: 0s - loss: 0.1402 - accuracy: 0.8321\n",
            "Epoch 87: val_accuracy did not improve from 0.83182\n",
            "3293/3293 [==============================] - 9s 3ms/step - loss: 0.1402 - accuracy: 0.8321 - val_loss: 0.1376 - val_accuracy: 0.8318\n",
            "Epoch 88/100\n",
            "3293/3293 [==============================] - ETA: 0s - loss: 0.1387 - accuracy: 0.8321\n",
            "Epoch 88: val_accuracy did not improve from 0.83182\n",
            "3293/3293 [==============================] - 9s 3ms/step - loss: 0.1387 - accuracy: 0.8321 - val_loss: 0.1376 - val_accuracy: 0.8318\n",
            "Epoch 89/100\n",
            "3289/3293 [============================>.] - ETA: 0s - loss: 0.1395 - accuracy: 0.8321\n",
            "Epoch 89: val_accuracy did not improve from 0.83182\n",
            "3293/3293 [==============================] - 8s 2ms/step - loss: 0.1395 - accuracy: 0.8321 - val_loss: 0.1376 - val_accuracy: 0.8318\n",
            "Epoch 90/100\n",
            "3291/3293 [============================>.] - ETA: 0s - loss: 0.1396 - accuracy: 0.8320\n",
            "Epoch 90: val_accuracy did not improve from 0.83182\n",
            "3293/3293 [==============================] - 8s 2ms/step - loss: 0.1395 - accuracy: 0.8321 - val_loss: 0.1376 - val_accuracy: 0.8318\n",
            "Epoch 91/100\n",
            "3278/3293 [============================>.] - ETA: 0s - loss: 0.1392 - accuracy: 0.8317\n",
            "Epoch 91: val_accuracy did not improve from 0.83182\n",
            "3293/3293 [==============================] - 9s 3ms/step - loss: 0.1388 - accuracy: 0.8321 - val_loss: 0.1376 - val_accuracy: 0.8318\n",
            "Epoch 92/100\n",
            "3288/3293 [============================>.] - ETA: 0s - loss: 0.1400 - accuracy: 0.8323\n",
            "Epoch 92: val_accuracy did not improve from 0.83182\n",
            "3293/3293 [==============================] - 9s 3ms/step - loss: 0.1402 - accuracy: 0.8321 - val_loss: 0.1376 - val_accuracy: 0.8318\n",
            "Epoch 93/100\n",
            "3290/3293 [============================>.] - ETA: 0s - loss: 0.1390 - accuracy: 0.8321\n",
            "Epoch 93: val_accuracy did not improve from 0.83182\n",
            "3293/3293 [==============================] - 8s 3ms/step - loss: 0.1391 - accuracy: 0.8321 - val_loss: 0.1376 - val_accuracy: 0.8318\n",
            "Epoch 94/100\n",
            "3277/3293 [============================>.] - ETA: 0s - loss: 0.1404 - accuracy: 0.8319\n",
            "Epoch 94: val_accuracy did not improve from 0.83182\n",
            "3293/3293 [==============================] - 8s 3ms/step - loss: 0.1399 - accuracy: 0.8321 - val_loss: 0.1376 - val_accuracy: 0.8318\n",
            "Epoch 95/100\n",
            "3273/3293 [============================>.] - ETA: 0s - loss: 0.1392 - accuracy: 0.8317\n",
            "Epoch 95: val_accuracy did not improve from 0.83182\n",
            "3293/3293 [==============================] - 8s 3ms/step - loss: 0.1387 - accuracy: 0.8321 - val_loss: 0.1376 - val_accuracy: 0.8318\n",
            "Epoch 96/100\n",
            "3293/3293 [==============================] - ETA: 0s - loss: 0.1384 - accuracy: 0.8321\n",
            "Epoch 96: val_accuracy did not improve from 0.83182\n",
            "3293/3293 [==============================] - 8s 2ms/step - loss: 0.1384 - accuracy: 0.8321 - val_loss: 0.1376 - val_accuracy: 0.8318\n",
            "Epoch 97/100\n",
            "3278/3293 [============================>.] - ETA: 0s - loss: 0.1386 - accuracy: 0.8324\n",
            "Epoch 97: val_accuracy did not improve from 0.83182\n",
            "3293/3293 [==============================] - 8s 2ms/step - loss: 0.1388 - accuracy: 0.8321 - val_loss: 0.1376 - val_accuracy: 0.8318\n",
            "Epoch 98/100\n",
            "3284/3293 [============================>.] - ETA: 0s - loss: 0.1395 - accuracy: 0.8322\n",
            "Epoch 98: val_accuracy did not improve from 0.83182\n",
            "3293/3293 [==============================] - 12s 4ms/step - loss: 0.1396 - accuracy: 0.8321 - val_loss: 0.1376 - val_accuracy: 0.8318\n",
            "Epoch 99/100\n",
            "3290/3293 [============================>.] - ETA: 0s - loss: 0.1395 - accuracy: 0.8321\n",
            "Epoch 99: val_accuracy did not improve from 0.83182\n",
            "3293/3293 [==============================] - 8s 3ms/step - loss: 0.1394 - accuracy: 0.8321 - val_loss: 0.1376 - val_accuracy: 0.8318\n",
            "Epoch 100/100\n",
            "3280/3293 [============================>.] - ETA: 0s - loss: 0.1388 - accuracy: 0.8322\n",
            "Epoch 100: val_accuracy did not improve from 0.83182\n",
            "3293/3293 [==============================] - 9s 3ms/step - loss: 0.1388 - accuracy: 0.8321 - val_loss: 0.1376 - val_accuracy: 0.8318\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f63ab41afd0>"
            ]
          },
          "metadata": {},
          "execution_count": 29
        }
      ],
      "source": [
        "# MLP\n",
        "mlp_model = Sequential()\n",
        "mlp_model.add(Dense(num_nodes, input_dim=x_train.shape[1], activation='sigmoid'))\n",
        "mlp_model.add(Dropout(dropout))\n",
        "\n",
        "for i in range(num_layers-1):\n",
        "    mlp_model.add(Dense(num_nodes, activation='sigmoid'))\n",
        "    mlp_model.add(Dropout(dropout))\n",
        "mlp_model.add(Dense(1, activation='sigmoid'))\n",
        "mlp_model.compile(loss=loss_fn, optimizer=optimizer, metrics=['accuracy'])\n",
        "es = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=2)\n",
        "mc = ModelCheckpoint('mlp_best_model.h5', monitor='val_accuracy', mode='max', verbose=1, save_best_only=True)\n",
        "mlp_model.fit(x_train, y_train,\n",
        "                batch_size=batch_size,\n",
        "                epochs=num_epochs,\n",
        "                validation_data=(x_val, y_val),\n",
        "                callbacks=[es,mc],\n",
        "                verbose=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YwnZpaIIKsH-"
      },
      "source": [
        "## 4f. Prediction using a stacking ensemble"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "Bt1Cbu4WKsH_"
      },
      "outputs": [],
      "source": [
        "# stacking ensemble\n",
        "\n",
        "# creating stacked dataset\n",
        "lasso_pred = best_lasso_model.predict(x_val)\n",
        "ridge_pred = best_ridge_model.predict(x_val)\n",
        "rf_pred = rf_model.predict(x_val)\n",
        "lgb_pred = lgb_model.predict(x_val)\n",
        "mlp_pred = mlp_model.predict(x_val)\n",
        "mlp_pred = mlp_pred.reshape(mlp_pred.shape[0])\n",
        "preds = [lasso_pred, ridge_pred, rf_pred, lgb_pred, mlp_pred]\n",
        "stacked_X = None\n",
        "for pred in preds:\n",
        "    if stacked_X is None:\n",
        "        stacked_X = pred\n",
        "    else:\n",
        "        stacked_X = np.dstack((stacked_X, pred))\n",
        "\n",
        "stacked_X = stacked_X.reshape(stacked_X.shape[1],stacked_X.shape[2])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "id": "fM66YZNmKsIB"
      },
      "outputs": [],
      "source": [
        "# train-test split\n",
        "cut = stacked_X.shape[0]//5\n",
        "stacked_x_val = stacked_X[:cut]\n",
        "stacked_x_train = stacked_X[cut:]\n",
        "stacked_y_val = y_val[:cut]\n",
        "stacked_y_train = y_val[cut:]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "id": "lnK6eKCBKsID",
        "outputId": "b6f4cd5b-3803-4e97-ac9c-547a7d140983",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100\n",
            "632/659 [===========================>..] - ETA: 0s - loss: 0.1483 - accuracy: 0.8307\n",
            "Epoch 1: val_accuracy improved from -inf to 0.82371, saving model to ensemble_best_model.h5\n",
            "659/659 [==============================] - 2s 2ms/step - loss: 0.1475 - accuracy: 0.8312 - val_loss: 0.1368 - val_accuracy: 0.8237\n",
            "Epoch 2/100\n",
            "637/659 [===========================>..] - ETA: 0s - loss: 0.1383 - accuracy: 0.8324\n",
            "Epoch 2: val_accuracy did not improve from 0.82371\n",
            "659/659 [==============================] - 1s 2ms/step - loss: 0.1380 - accuracy: 0.8338 - val_loss: 0.1365 - val_accuracy: 0.8237\n",
            "Epoch 3/100\n",
            "638/659 [============================>.] - ETA: 0s - loss: 0.1410 - accuracy: 0.8362\n",
            "Epoch 3: val_accuracy did not improve from 0.82371\n",
            "659/659 [==============================] - 1s 2ms/step - loss: 0.1411 - accuracy: 0.8338 - val_loss: 0.1364 - val_accuracy: 0.8237\n",
            "Epoch 4/100\n",
            "656/659 [============================>.] - ETA: 0s - loss: 0.1170 - accuracy: 0.8403\n",
            "Epoch 4: val_accuracy improved from 0.82371 to 0.84498, saving model to ensemble_best_model.h5\n",
            "659/659 [==============================] - 1s 2ms/step - loss: 0.1167 - accuracy: 0.8407 - val_loss: 0.0721 - val_accuracy: 0.8450\n",
            "Epoch 5/100\n",
            "648/659 [============================>.] - ETA: 0s - loss: 0.0713 - accuracy: 0.8515\n",
            "Epoch 5: val_accuracy did not improve from 0.84498\n",
            "659/659 [==============================] - 1s 2ms/step - loss: 0.0713 - accuracy: 0.8513 - val_loss: 0.0660 - val_accuracy: 0.8450\n",
            "Epoch 6/100\n",
            "658/659 [============================>.] - ETA: 0s - loss: 0.0711 - accuracy: 0.8503\n",
            "Epoch 6: val_accuracy did not improve from 0.84498\n",
            "659/659 [==============================] - 1s 2ms/step - loss: 0.0710 - accuracy: 0.8505 - val_loss: 0.0651 - val_accuracy: 0.8450\n",
            "Epoch 7/100\n",
            "641/659 [============================>.] - ETA: 0s - loss: 0.0705 - accuracy: 0.8506\n",
            "Epoch 7: val_accuracy did not improve from 0.84498\n",
            "659/659 [==============================] - 1s 2ms/step - loss: 0.0706 - accuracy: 0.8513 - val_loss: 0.0668 - val_accuracy: 0.8450\n",
            "Epoch 8/100\n",
            "645/659 [============================>.] - ETA: 0s - loss: 0.0714 - accuracy: 0.8500\n",
            "Epoch 8: val_accuracy did not improve from 0.84498\n",
            "659/659 [==============================] - 1s 2ms/step - loss: 0.0706 - accuracy: 0.8513 - val_loss: 0.0655 - val_accuracy: 0.8450\n",
            "Epoch 9/100\n",
            "657/659 [============================>.] - ETA: 0s - loss: 0.0722 - accuracy: 0.8505\n",
            "Epoch 9: val_accuracy did not improve from 0.84498\n",
            "659/659 [==============================] - 1s 2ms/step - loss: 0.0722 - accuracy: 0.8505 - val_loss: 0.0671 - val_accuracy: 0.8450\n",
            "Epoch 9: early stopping\n"
          ]
        }
      ],
      "source": [
        "# use MLP for stacking ensemble\n",
        "model = Sequential()\n",
        "model.add(Dense(num_nodes, input_dim=stacked_x_train.shape[1], activation='sigmoid'))\n",
        "model.add(Dropout(dropout))\n",
        "model.add(Dense(1, activation='sigmoid'))\n",
        "model.compile(loss=keras_rmse, optimizer='adam', metrics=['accuracy'])\n",
        "es = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=3)\n",
        "mc = ModelCheckpoint('ensemble_best_model.h5', monitor='val_accuracy', mode='max', verbose=1, save_best_only=True)\n",
        "history=model.fit(stacked_x_train, stacked_y_train,\n",
        "                batch_size=batch_size,\n",
        "                epochs=100,\n",
        "                validation_data=(stacked_x_val, stacked_y_val),\n",
        "               callbacks=[es,mc],\n",
        "                 verbose=1)\n",
        "# score, accuracy = model.evaluate(stacked_x_test, stacked_y_test,batch_size=batch_size)\n",
        "# # print(f'Test score: {score}')\n",
        "# print(f'Final test accuracy: {accuracy}')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "hist = history\n",
        "plt.plot(hist.history[\"accuracy\"])\n",
        "plt.plot(hist.history[\"val_accuracy\"])\n",
        "plt.plot(hist.history[\"loss\"])\n",
        "plt.plot(hist.history[\"val_loss\"])\n",
        "plt.title(\"model_accuracy\")\n",
        "plt.ylabel(\"Accuracy\")\n",
        "plt.xlabel(\"Epoch\")\n",
        "plt.legend([\"Accuracy\",\"Validation Accuracy\",\"loss\",\"validation loss\"])\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 295
        },
        "id": "plurXwvYU48F",
        "outputId": "8f80c5f9-32a0-43b4-90cd-b25fd2abe070"
      },
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYIAAAEWCAYAAABrDZDcAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAgAElEQVR4nO3de3wU5fX48c/ZzZ1ACILIJTbBgiCEcMeKEBC1eEMRKSCoaMVqqxZt/ZWvbdFa/da2fq3V+vVb71owgKgUK0qrXK03LiIKSuUSTAC5JyQkIcnu+f0xk2UTNiGBLJuw5+1rXzvzzDPPnN3gnJlnZp8RVcUYY0z08kQ6AGOMMZFlicAYY6KcJQJjjIlylgiMMSbKWSIwxpgoZ4nAGGOinCUCExVE5EURebCedXNF5MJwx2RMU2GJwBhjopwlAmOaIRGJiXQM5tRhicA0KW63zD0isk5EDonIcyLSXkTeFpEiEXlXRFLduqNFZL2IFIjIUhHpEdROXxFZ464zB0iosZ3LRWStu+4HItK7gXEOEpEP3fV3ishfRCQuaHlPEfmXiOwXkV0icq9b7hWRe0VksxvbahFJE5F0EdHgHbz7mW52p6eIyL9F5E8isg+4X0TOEpHFIrJPRPaKyCwRaR20fpqIvC4ie9w6fxGRODemzKB6p4tIiYi0a8h3YE4dlghMUzQWuAjoBlwBvA3cC7TD+Td7p4h0A3KAaW75QuBNd0cXB8wH/ga0AV512wScJAE8D/wIOA34K7BAROIbEKMPuAtoC3wPGAn82G2/JfAu8A7QEfgu8J673t3AROBSoBVwE1BSz20OBrYA7YGHAAF+526jB5AG3O/G4AX+AWwD0oFOwGxVLQdmA5OD2p0IvKeqe+r96c2pRVXtZa8m8wJygUlB868BTwXN34Gzk/81MDeo3ANsB4YDw4AdgAQt/wB40J1+Cvhtje1uBLKDYriwgXFPA95wpycCn9ZSbyNwZYjydECBmKCypcDN7vQU4JtjxHBV1XZxktOe4PaC6g0Gvqn6foBVwA8i/be3V+Re1s9omqJdQdOlIeaTcY6Ct1UVqqpfRPJwjnx9wHZVDR5RcVvQ9HeAG0TkjqCyOLfNenHPSB4FBgBJQAyw2l2cBmyuZdW6lh1LXo0Y2gN/BoYCLXGS4YGg7WxT1cqajajqxyJSAgwXkZ04ZywLjjMmcwqwriHTXO3A2aEDICKCs/PbDuwEOrllVc4Mms4DHlLV1kGvJFXNacD2nwK+Arqqaiucrquq7eUBXWpZLw84K0T5Ifc9KajsjBp1ag4V/N9uWaYbw+QaMZxZx0Xll9z61wHzVLWslnomClgiMM3VXOAyERkpIrHAz4DDOF1AHwKVONcSYkXkamBQ0LrPALeKyGBxtBCRy9y+/fpqCRwEikWkO3Bb0LJ/AB1EZJqIxItISxEZ7C57FvitiHR1t91bRE5Tp39+OzDZvaB8E6ETRs0YioFCEekE3BO07BOchPiw+/kSRGRI0PKZwBicZPByAz63OQVZIjDNkqpuxNmJPQHsxbmofIWqlqtzQfRqnH71/cB44PWgdVcBU4G/4HSlbHLrNsTPgWuBIpzEMieo/SKci91XAN8CXwMj3MWP4iSxf+IkkueARHfZVJyd+T6gJ05Sq8tvgH5AIfBWjc/oc7f/XZzrAfk430PV8jxgDc4ZxYoGfG5zCpLq3ajGmGghIs8DO1T1V5GOxUSWXSw2JgqJSDrOWVPfyEZimgLrGjKmFu6P2IpDvO6NdGwnQkR+C3wB/FFVt0Y6HhN51jVkjDFRzs4IjDEmyjW7awRt27bV9PT0SIdhjDHNyurVq/eqasjxpJpdIkhPT2fVqlWRDsMYY5oVEdlW2zLrGjLGmChnicAYY6KcJQJjjIlylgiMMSbKWSIwxpgoZ4nAGGOinCUCY4yJcs3udwTGRFJ5pZ+C0nIKSyooKK2goKSCwtIKCkrKOVhWCaqICCIgCB7BmQ4qc97B45YRNC04dT3ORLV6bpHTVlC9au0GlrnrudOqznjTftXAtAamnXe/umXVluEuc8pRrVHPWUaNesHtVrXlDy6rWqEJq/k387jfLxz9N/G45YG/SfD6tfxNgv+OAng8of+Owf+OenZMIa1NUsh4T4QlAhN1VJWyCmeHXlBStTN3p0trzLtlhSXlFJRWUFLui3T4p4xqz49rYppqjnrwql5MPvc7x67YQJYITNOkCsW7YP+Wo1+F20H9zpEnNY9eQx+xVj9CdabjgfbuqybPUUeDgsThvAJH4iGO7jj6eZJ1fswGLNTaFhxrdSXwAMv673sldP06GmjIfr0J54CApvh3LJPfADc0ILL6sURgTgpV5XCln9JyH4fKK533wxVUHshHDmzFW7CV+IO5JBR9Q8uSb2hVmkec/8hjdH142O09gx2eDuzQ/pT4hMOVfuoaPTfG4yE+xkNcjPPuTHuJj/UQ5/UQH+sh3ushLtZLvDsfF+MhxuM5oR3VqbZDjFZN8e+YfEbXsLRrieAkUVUq/YrPr1T4/O67M+9rouehfr9SUu6jpLySknIfhw5XUlrh49DhI2VVyw8d9lFa4b4H7exLDx8mpXw3p1du50y+5Tuyi3TZxXfkW3rIbuKlIrC9wxpDnp7OWm1PHiPY4e3AnthO7I3rRHF8B+Li42kR5yUpPobWibG0ToqldWIcKUmx7nycWxZLq8RYEmK9Efz2jGk+oiYRfJZXwEdb9lHpVyp9SqXf7077g3bQis/vd5e7dQLTR+pW1tiRV/id+UC7vhr13fZPRXFeD63ilLNi99PFu5s+nm9J41s6+XfSvnInp1XuJEYqIdapX+mJp7jFmZQmd+fblFFUpmQgbTLwtD2LuNQ02iXEc2acl7gYu6HNmJMlahLBR1v28bu3v6pWFusVYjweYjxCjFfwejzEegWvR4j1evB6JLAsuF5SXIxb5pR7vUKsp5b1g+o563uC2nTW8bp3CzQ1IpAUF0NSvJcWnkpSD+8gueQbWhz6hviD24gtzMVzYAsU5EHwRdS4ZGiTAW36QZsu1V4xyWfQ2uOhdeQ+ljGmhqhJBDd23MZNQ5YgHudC34n1AjeAAj731dxUlsKBXNi/FQrzqXZxKz4FTusCnfpD5rjqO/wW7Zr2LSHGmGqiJhHE7f8a1r8a6TCaF28spKbDd8476siexFTb2RtzioiaRMDgHzkvY4wx1dgVOWOMiXJhTQQiMkpENorIJhGZHmL5mSKyREQ+FZF1InJpOOMxxhhztLAlAhHxAk8ClwDnABNF5Jwa1X4FzFXVvsAE4H/DFY8xxpjQwnlGMAjYpKpbVLUcmA1cWaOOAq3c6RRgRxjjMcYYE0I4E0EnIC9oPt8tC3Y/MFlE8oGFwB2hGhKRW0RklYis2rNnTzhiNcaYqBXpi8UTgRdVtTNwKfA3ETkqJlV9WlUHqOqAdu3anfQgjTHmVBbORLAdSAua7+yWBfshMBdAVT8EEoC2YYzJGGNMDeFMBCuBriKSISJxOBeDF9So8w0wEkBEeuAkAuv7McaYkyhsiUBVK4HbgUXAlzh3B60XkQdEZLRb7WfAVBH5DMgBpmhd4wobY4xpdGH9ZbGqLsS5CBxcNiNoegMwJJwxGGOMqVukLxYbY4yJMEsExhgT5SwRGGNMlLNEYIwxUc4SgTHGRDlLBMYYE+UsERhjTJSzRGCMMVHOEoExxkQ5SwTGGBPlLBEYY0yUs0RgjDFRzhKBMcZEOUsExhgT5SwRGGNMlLNEYIwxUS6siUBERonIRhHZJCLTQyz/k4isdV//EZGCcMZjjDHmaGF7QpmIeIEngYuAfGCliCxwn0oGgKreFVT/DqBvuOIxxhgTWjjPCAYBm1R1i6qWA7OBK+uoPxHnucXGGGNOonAmgk5AXtB8vlt2FBH5DpABLK5l+S0iskpEVu3Zs6fRAzXGmGjWVC4WTwDmqaov1EJVfVpVB6jqgHbt2p3k0Iwx5tQWzkSwHUgLmu/sloUyAesWMsaYiAhnIlgJdBWRDBGJw9nZL6hZSUS6A6nAh2GMxRhjTC3ClghUtRK4HVgEfAnMVdX1IvKAiIwOqjoBmK2qGq5YjDHG1C5st48CqOpCYGGNshk15u8PZwzGGGPq1lQuFhtjjIkQSwTGGBPlLBEYY0yUs0RgjDFRzhKBMcZEOUsExhgT5SwRGGNMlLNEYIwxUc4SgTHGRDlLBMYYE+UsERhjTJSzRGCMMVHOEoExxkQ5SwTGGBPlLBEYY0yUs0RgjDFRLqyJQERGichGEdkkItNrqfMDEdkgIutF5JVwxmOMMeZoYXtCmYh4gSeBi4B8YKWILFDVDUF1ugL/BQxR1QMicnq44jHGGBNaOM8IBgGbVHWLqpYDs4Era9SZCjypqgcAVHV3GOMxxhgTQjgTQScgL2g+3y0L1g3oJiL/FpGPRGRUqIZE5BYRWSUiq/bs2ROmcI0xJjpF+mJxDNAVGA5MBJ4RkdY1K6nq06o6QFUHtGvX7iSHaIwxp7ZwJoLtQFrQfGe3LFg+sEBVK1R1K/AfnMRgjDHmJAlnIlgJdBWRDBGJAyYAC2rUmY9zNoCItMXpKtoSxpiMMcbUELZEoKqVwO3AIuBLYK6qrheRB0RktFttEbBPRDYAS4B7VHVfuGIyxhhzNFHVSMfQIAMGDNBVq1ZFOgxjIq6iooL8/HzKysoiHYppQhISEujcuTOxsbHVykVktaoOCLVO2H5HYIwJr/z8fFq2bEl6ejoiEulwTBOgquzbt4/8/HwyMjLqvV6k7xoyxhynsrIyTjvtNEsCJkBEOO200xp8lmiJwJhmzJKAqel4/k1YIjDGnJD58+cjInz11VeRDsUcJ0sExpgTkpOTw/nnn09OTk7YtuHz+cLWtrFEYIw5AcXFxbz//vs899xzzJ49G3B22j//+c/p1asXvXv35oknngBg5cqVnHfeeWRlZTFo0CCKiop48cUXuf322wPtXX755SxduhSA5ORkfvazn5GVlcWHH37IAw88wMCBA+nVqxe33HILVXc8btq0iQsvvJCsrCz69evH5s2buf7665k/f36g3UmTJvH3v//9JH0rzY/dNWTMKeA3b65nw46DjdrmOR1bcd8VPeus8/e//51Ro0bRrVs3TjvtNFavXs0nn3xCbm4ua9euJSYmhv3791NeXs748eOZM2cOAwcO5ODBgyQmJtbZ9qFDhxg8eDD/8z//48RzzjnMmDEDgOuuu45//OMfXHHFFUyaNInp06czZswYysrK8Pv9/PCHP+RPf/oTV111FYWFhXzwwQe89NJLjfPFnILsjMAYc9xycnKYMGECABMmTCAnJ4d3332XH/3oR8TEOMeZbdq0YePGjXTo0IGBAwcC0KpVq8Dy2ni9XsaOHRuYX7JkCYMHDyYzM5PFixezfv16ioqK2L59O2PGjAGce+iTkpLIzs7m66+/Zs+ePeTk5DB27Nhjbi+a2TdjzCngWEfu4bB//34WL17M559/jojg8/kQkcDOvj5iYmLw+/2B+eDbHhMSEvB6vYHyH//4x6xatYq0tDTuv//+Y94ief311zNz5kxmz57NCy+80MBPF13sjMAYc1zmzZvHddddx7Zt28jNzSUvL4+MjAyysrL461//SmVlJeAkjLPPPpudO3eycuVKAIqKiqisrCQ9PZ21a9fi9/vJy8vjk08+Cbmtqp1+27ZtKS4uZt68eQC0bNmSzp07B64HHD58mJKSEgCmTJnCY489BjjdSqZ2x0wEInKFiFjCMMZUk5OTE+iSqTJ27Fh27tzJmWeeSe/evcnKyuKVV14hLi6OOXPmcMcdd5CVlcVFF11EWVkZQ4YMISMjg3POOYc777yTfv36hdxW69atmTp1Kr169eL73/9+tbOOv/3tbzz++OP07t2b8847j2+//RaA9u3b06NHD2688cbwfQmniGOONSQiM4HvAa8Bz6tqRG8WtrGGjHF8+eWX9OjRI9JhNFklJSVkZmayZs0aUlJSIh3OSRXq30ZdYw0d80hfVScDfYHNwIsi8qH7xLCWjRGwMcY0tnfffZcePXpwxx13RF0SOB71ulisqgdFZB6QCEwDxgD3iMjjqvpEOAM0xpiGuvDCC9m2bVukw2g26nONYLSIvAEsBWKBQap6CZAF/Cy84RljjAm3+pwRjAX+pKrLgwtVtUREfhiesIwxxpws9bkb6H4gcE+XiCSKSDqAqr5X14oiMkpENorIJhGZHmL5FBHZIyJr3dfNDYreGGPMCatPIngV8AfN+9yyOomIF3gSuAQ4B5goIqFu5p2jqn3c17P1iMcYY0wjqk8iiFHV8qoZdzquHusNAjap6hZ3ndnAlccXpjGmqRkxYgSLFi2qVvbYY49x22231brO8OHDqbr9+9JLL6WgoOCoOvfffz+PPPJIndueP38+GzZsCMzPmDGDd999tyHh12natGl06tSp2q+eT2X1SQR7gh42j4hcCeytx3qdgLyg+Xy3rKaxIrJOROaJSFo92jXGNAETJ04MjDhaZfbs2UycOLFe6y9cuJDWrVsf17ZrJoIHHniACy+88Ljaqsnv9/PGG2+QlpbGsmXLGqXNUKp+ed0U1CcR3ArcKyLfiEge8AvgR420/TeBdFXtDfwLCDk8oPu7hVUismrPnj2NtGljzIm45ppreOuttygvdzoMcnNz2bFjB0OHDuW2225jwIAB9OzZk/vuuy/k+unp6ezd6xxTPvTQQ3Tr1o3zzz+fjRs3Buo888wzDBw4kKysLMaOHUtJSQkffPABCxYs4J577qFPnz5s3ryZKVOmBIadeO+99+jbty+ZmZncdNNNHD58OLC9++67j379+pGZmVnrg3SWLl1Kz549ue2226o9Y2HXrl2MGTOGrKwssrKy+OCDDwB4+eWXA7+ivu666wCqxQPOkNpVbQ8dOpTRo0cHhr246qqr6N+/Pz179uTpp58OrPPOO+/Qr18/srKyGDlyJH6/n65du1K1D/T7/Xz3u9+lMfaJx7xrSFU3A+eKSLI7X1zPtrcDwUf4nd2y4Lb3Bc0+C/yhlhieBp4G55fF9dy+MdHj7enw7eeN2+YZmXDJw7UubtOmDYMGDeLtt9/myiuvZPbs2fzgBz9ARHjooYdo06YNPp+PkSNHsm7dOnr37h2yndWrVzN79mzWrl1LZWUl/fr1o3///gBcffXVTJ06FYBf/epXPPfcc9xxxx2MHj2ayy+/nGuuuaZaW2VlZUyZMoX33nuPbt26cf311/PUU08xbdo0wBmraM2aNfzv//4vjzzyCM8+e/RlyZycHCZOnMiVV17JvffeS0VFBbGxsdx5551kZ2fzxhtv4PP5KC4uZv369Tz44IN88MEHtG3blv379x/za12zZg1ffPFF4OHyzz//PG3atKG0tJSBAwcyduxY/H4/U6dOZfny5WRkZLB//348Hg+TJ09m1qxZTJs2jXfffZesrCzatWt3zG0eS73GEBKRy4AfA3eLyAwRmVGP1VYCXUUkQ0TigAnAghrtdgiaHQ18Wb+wjTFNQXD3UHC30Ny5c+nXrx99+/Zl/fr11bpxalqxYgVjxowhKSmJVq1aMXp0oCeaL774gqFDh5KZmcmsWbNYv359nfFs3LiRjIwMunXrBsANN9zA8uVH7ny/+uqrAejfvz+5ublHrV9eXs7ChQu56qqraNWqFYMHDw5cB1m8eHHg+ofX6yUlJYXFixczbtw42rZtCzjJ8VgGDRoUSAIAjz/+OFlZWZx77rnk5eXx9ddf89FHHzFs2LBAvap2b7rpJl5++WXASSCNNY7SMc8IROT/gCRgBM5R+zUE3U5aG1WtFJHbgUWAF2ecovUi8gCwSlUXAHe61x8qgf3AlOP9IMZEtTqO3MPpyiuv5K677mLNmjWUlJTQv39/tm7dyiOPPMLKlStJTU1lypQpxxwyujZTpkxh/vz5ZGVl8eKLLwaeXna84uPjAWdHHqqPftGiRRQUFJCZmQk44xUlJiZy+eWXN2g7wcNr+/3+QPcZQIsWLQLTS5cu5d133+XDDz8kKSmJ4cOH1/ldpaWl0b59exYvXswnn3zCrFmzGhRXbepzRnCeql4PHFDV3+AMQNetPo2r6kJV7aaqZ6nqQ27ZDDcJoKr/pao9VTVLVUdEekA7Y0zDJCcnM2LECG666abA2cDBgwdp0aIFKSkp7Nq1i7fffrvONoYNG8b8+fMpLS2lqKiIN998M7CsqKiIDh06UFFRUW2n17JlS4qKio5q6+yzzyY3N5dNmzYBzsik2dnZ9f48OTk5PPvss+Tm5pKbm8vWrVv517/+RUlJCSNHjuSpp54CnMdxFhYWcsEFF/Dqq6+yb5/Ty13VNZSens7q1asBWLBgARUVFSG3V1hYSGpqKklJSXz11Vd89NFHAJx77rksX76crVu3VmsX4Oabb2by5MmMGzcu8LyGE1WfRFCVnkpEpCNQAXSoo74xJopMnDiRzz77LJAIsrKy6Nu3L927d+faa69lyJAhda7fr18/xo8fT1ZWFpdcckm1IaZ/+9vfMnjwYIYMGUL37t0D5RMmTOCPf/wjffv2ZfPmzYHyhIQEXnjhBcaNG0dmZiYej4dbb721Xp+jpKSEd955h8suuyxQ1qJFC84//3zefPNN/vznP7NkyRIyMzPp378/GzZsoGfPnvzyl78kOzubrKws7r77bgCmTp3KsmXLAs9bDj4LCDZq1CgqKyvp0aMH06dP59xzzwWgXbt2PP3001x99dVkZWUxfvz4wDqjR4+muLi4UYfXrs8w1L8GngBG4vxATIFnVLU+1wkanQ1DbYzDhqGOTqtWreKuu+5ixYoVtdZp6DDUdV4jcB9I856qFgCvicg/gARVLWxw9MYYY07Iww8/zFNPPdVo1waq1Nk1pKp+nLOAqvnDlgSMMSYypk+fzrZt2zj//PMbtd36XCN4T0TGiog06paNMcY0CfVJBD/CGWTusIgcFJEiETkY5riMMcacJPX5ZbE9ktIYY05h9flB2bBQ5TUfVGOMMaZ5qs8Tyu4Jmk7AGV56NXBBWCIyxjQbycnJFBfXd/gx01TVp2voiuB5d6jox8IWkTHGmJOqXoPO1ZAP2K9YjDEBqso999xDr169yMzMZM6cOQDs3LmTYcOG0adPH3r16sWKFSvw+XxMmTIlUPdPf/pThKM39blG8ATOr4nBSRx9gDXhDMoY0zC//+T3fLW/cYfq6t6mO78Y9It61X399ddZu3Ytn332GXv37mXgwIEMGzaMV155he9///v88pe/xOfzUVJSwtq1a9m+fTtffPEFQMinlJmTqz7XCILHc6gEclT132GKxxjTDL3//vtMnDgRr9dL+/btyc7OZuXKlQwcOJCbbrqJiooKrrrqKvr06UOXLl3YsmULd9xxB5dddhkXX3xxpMOPevVJBPOAMlX1gfNQehFJUtWS8IZmjKmv+h65n2zDhg1j+fLlvPXWW0yZMoW7776b66+/ns8++4xFixbxf//3f8ydO5fnn38+0qFGtXr9shhIDJpPBBrvKdHGmGZv6NChzJkzB5/Px549e1i+fDmDBg1i27ZttG/fnqlTp3LzzTezZs0a9u7di9/vZ+zYsTz44IOsWWM9zZFWnzOChODHU6pqsYgkhTEmY0wzM2bMGD788EOysrIQEf7whz9wxhln8NJLL/HHP/6R2NhYkpOTefnll9m+fTs33nhj4MEtv/vd7yIcvanPMNT/Bu5Q1TXufH/gL6r6vZMQ31FsGGpjHDYMtalNQ4ehrk/X0DTgVRFZISLvA3OA2+sTjIiMEpGNIrJJRKbXUW+siKiIhAzSGGNM+NTnB2UrRaQ7cLZbtFFVQz93LYiIeHGGsL4I57cHK0VkgapuqFGvJfBT4OOGBm+MMebEHfOMQER+ArRQ1S9U9QsgWUR+XI+2BwGbVHWLqpYDs4ErQ9T7LfB7jjwS0xhjzElUn66hqe4TygBQ1QPA1Hqs1wnIC5rPd8sCRKQfkKaqb9XVkIjcIiKrRGTVnj176rFpY4wx9VWfROANfiiN2+UTd6Ibdh+D+Sjws2PVVdWnVXWAqg5o167diW7aGGNMkPrcPvoOMEdE/urO/wh4ux7rbQfSguY7u2VVWgK9gKVunjkDWCAio1XVbgsyxpiTpD5nBL8AFgO3uq/Pqf4Ds9qsBLqKSIaIxAETgAVVC1W1UFXbqmq6qqYDHwGWBIw5hSUnJwOwY8cOrrnmmpB1hg8fzrFuEX/ssccoKTkyuMGll17aKGMW3X///TzyyCMn3E5zc8xE4D7A/mMgF+cC8AXAl/VYrxLnNtNFbv25qrpeRB4QkdEnErQxpnnr2LEj8+bNO+71ayaChQsX0rp168YILSrVmghEpJuI3CciXwFPAN8AqOoIVf1LfRpX1YWq2k1Vz1LVh9yyGaq6IETd4XY2YEzzMX36dJ588snAfNXRdHFxMSNHjqRfv35kZmby97///ah1c3Nz6dWrFwClpaVMmDCBHj16MGbMGEpLSwP1brvtNgYMGEDPnj257777AHj88cfZsWMHI0aMYMSIEQCkp6ezd+9eAB599FF69epFr169eOyxxwLb69GjB1OnTqVnz55cfPHF1bYTytq1azn33HPp3bs3Y8aM4cCBA4Htn3POOfTu3ZsJEyYAsGzZMvr06UOfPn3o27cvRUVFx/WdRkpd1wi+AlYAl6vqJgARueukRGWMaZBv//u/Ofxl4w5DHd+jO2fce2+ty8ePH8+0adP4yU9+AsDcuXNZtGgRCQkJvPHGG7Rq1Yq9e/dy7rnnMnr0aILuOanmqaeeIikpiS+//JJ169bRr1+/wLKHHnqINm3a4PP5GDlyJOvWrePOO+/k0UcfZcmSJbRt27ZaW6tXr+aFF17g448/RlUZPHgw2dnZpKam8vXXX5OTk8MzzzzDD37wA1577TUmT55c6+e7/vrreeKJJ8jOzmbGjBn85je/4bHHHuPhhx9m69atxMfHB7qjHnnkEZ588kmGDBlCcXExCQkJ9f6em4K6uoauBnYCS0TkGREZCYT+Sxpjok7fvn3ZvXs3O3bs4LPPPiM1NZW0tDRUlXvvvZfevXtz4YUXsn37dnbt2ln5tR0AABU4SURBVFVrO8uXLw/skHv37k3v3r0Dy+bOnUu/fv3o27cv69evZ8OGDbU1AzjDYY8ZM4YWLVqQnJzM1VdfzYoVKwDIyMigT58+APTv35/c3Nxa2yksLKSgoIDs7GwAbrjhBpYvXx6IcdKkScycOZOYGOdYesiQIdx99908/vjjFBQUBMqbi1qjVdX5wHwRaYHzQ7BpwOki8hTwhqr+8yTFaIw5hrqO3MNp3LhxzJs3j2+//Zbx48cDMGvWLPbs2cPq1auJjY0lPT2dsrKG/15069atPPLII6xcuZLU1FSmTJlyXO1UiY+PD0x7vd5jdg3V5q233mL58uW8+eabPPTQQ3z++edMnz6dyy67jIULFzJkyBAWLVpE9+7djzvWk60+F4sPqeor7rOLOwOf4txJZIyJcuPHj2f27NnMmzePcePGAc7R9Omnn05sbCxLlixh27ZtdbZR9SQzgC+++IJ169YBcPDgQVq0aEFKSgq7du3i7beP3LXesmXLkP3wQ4cOZf78+ZSUlHDo0CHeeOMNhg4d2uDPlZKSQmpqauBs4m9/+xvZ2dn4/X7y8vIYMWIEv//97yksLKS4uJjNmzeTmZnJL37xCwYOHMhXXzVuN124Nej8xf1V8dPuyxgT5Xr27ElRURGdOnWiQ4cOAEyaNIkrrriCzMxMBgwYcMwj49tuu40bb7yRHj160KNHD/r37w9AVlYWffv2pXv37qSlpTFkyJDAOrfccgujRo2iY8eOLFmyJFDer18/pkyZwqBBgwC4+eab6du3b53dQLV56aWXuPXWWykpKaFLly688MIL+Hw+Jk+eTGFhIarKnXfeSevWrfn1r3/NkiVL8Hg89OzZk0suuaTB24ukYw5D3dTYMNTGOGwYalObcAxDbYwx5hRmicAYY6KcJQJjjIlylgiMacaa2zU+E37H82/CEoExzVRCQgL79u2zZGACVJV9+/Y1+JfNzevnb8aYgM6dO5Ofn489rMkES0hIoHPnzg1axxKBMc1UbGwsGRkZkQ7DnAKsa8gYY6KcJQJjjIlylgiMMSbKhTURiMgoEdkoIptEZHqI5beKyOcislZE3heRc8IZjzHGmKOFLRGIiBd4ErgEOAeYGGJH/4qqZqpqH+APwKPhiscYY0xo4TwjGARsUtUtqloOzMZ5rkGAqh4Mmm0B2A3RxhhzkoXz9tFOQF7QfD4wuGYlEfkJcDcQB1wQqiERuQW4BeDMM89s9ECNMSaaRfxisao+qapn4Tzs5le11HlaVQeo6oB27dqd3ACNMeYUF85EsB1IC5rv7JbVZjZwVRjjMcYYE0I4E8FKoKuIZIhIHDABWBBcQUS6Bs1eBnwdxniMMcaEELZrBKpaKSK3A4sAL/C8qq4XkQeAVaq6ALhdRC4EKoADwA3hiscYY0xoYR1rSFUXAgtrlM0Imv5pOLdvjDHm2CJ+sdgYY0xkWSIwxpgoZ4nAGGOinCUCY4yJcpYIjDEmylkiMMaYKGeJwBhjopwlAmOMiXKWCIwxJspZIjDGmChnicAYY6KcJQJjjIlylgiMMSbKWSIwxpgoZ4nAGGOinCUCY4yJcmFNBCIySkQ2isgmEZkeYvndIrJBRNaJyHsi8p1wxmOMMeZoYUsEIuIFngQuAc4BJorIOTWqfQoMUNXewDzgD+GKxxhjTGjhPCMYBGxS1S2qWg7MBq4MrqCqS1S1xJ39COgcxniMMcaEEM5E0AnIC5rPd8tq80Pg7VALROQWEVklIqv27NnTiCEaY4xpEheLRWQyMAD4Y6jlqvq0qg5Q1QHt2rU7ucEZY8wpLiaMbW8H0oLmO7tl1YjIhcAvgWxVPRzGeIwxxoQQzjOClUBXEckQkThgArAguIKI9AX+CoxW1d1hjMUYY0wtwpYIVLUSuB1YBHwJzFXV9SLygIiMdqv9EUgGXhWRtSKyoJbmjDHGhEk4u4ZQ1YXAwhplM4KmLwzn9o0xxhxbk7hYbIwxJnIsERhjTJSzRGCMMVHOEoExxkQ5SwTGGBPlLBEYY0yUC+vto03Jxzs/Znn+clrGtaRlXEtaxbU6arpVXCsSYxIRkUiHa4wxJ03UJIKvD3zNq/95ldLK0jrrecUbSBBHJY3Y6uUp8SlHlVsiMcY0N6KqkY6hQQYMGKCrVq067vUr/BUUlxdTVF5EUXkRB8sPBqarzVcUcfBw9WVFFUXHTCQxEnNUIqntDCT4LKRSK6n0V39V+CuOntZKKnxHpkOtE7xeVZ0KX0WD6lf6K/GIhzNbnkmX1l3okuK+WnehQ4sOeMR6FY1pTkRktaoOCLUsas4IDrz6Kvufe/5IgQgJQAJwetURfPCRfPCkCJAEJKGAX/348Dvv6rz71edM48fnr8Sne/GzG5/68Pv9VOJz6uEPtHvQfQEclY6Dtl+1zOu+4kKdcIgQ+E/cF+KUBy2rNl9zGkHEE2irIsHLqqwDLEpfwuuVrwc2lRiTSHqrdDJSMjir9VmBJJHWKo1YT+wx/hLGmKYmahJBzGltSTin6gFpR3a7gTOi4D1x8FlSyOkQ69enDVX86neOun1Hjrz9WgkIHndHLO6cJ8TO3aO4ddwyFURwdvI1t1tXrDXjDbFuxfbtdPzbZsa0a0vi2BvYfXEftsQcYEvBFrYWbuXT3Z+ycOuREURiJIa0VmnVzh66pHQhvVU6SbFJGGOapqjrGjL1p34/h95/n/0zZ3Jo+QqIjaXVxReTOnkSiX36ICKUVJSwtXArWwq3OK8C5z2vKA+f+gJtdWzRkYzWGXRJ6cJZKWcFkkRKfEoEP6Ex0aOuriFLBKZeynNzOZCTQ8Frr+MvLiahZ09SJ02i1WWX4omPP6p+ha+CbQe3HUkQbpLIPZjLYd+Rx060SWhz1BlEl5QunJ50ul10N6YRWSIwjcZ/6BCFCxawf9YsyjdtxpuaSutx40idOIHYDh2Oub7P72PHoR3OWYR79rC5cDNbC7ZSVFEUqJccm0xGSsZR1yE6JXfC6/GG8yMac0qyRGAanapS8vHH7J85k+LFS0CEliNHkjppEkmDBjb4aF5V2Vu6N3D2sLlgc6DLaW/p3kC9OE8cXVp34byO5zE8bTi92/a2xGBMPVgiMGFVnr+dgtk5FLw6D19hIfHdupE6aRIpV1yOJ+nELxIXHi5ka+FWthZuZXPBZjbs38Cnuz6lUitJjU9laOehZHfO5ryO55Ecl9wIn8iYU0/EEoGIjAL+jHPX47Oq+nCN5cOAx4DewARVnXesNi0RNF3+sjIOvvUW+2fO4vCXX+Jp1YrWV19N6qRriUtLO3YDDXCw/CAfbP+ApflLWZG/goPlB4nxxDCg/QCGpw0nu3M2nVt2btRtGtOcRSQRiIgX+A9wEZCP8wzjiaq6IahOOtAK+DmwwBLBqUFVKV2zhv0zZ1L0z3+B309ydjapkyfT4rzvIZ7G/TFapb+StbvXsix/Gcvyl7G1cCsA3239XbI7ZzM8bTiZbTOtC8lEtUglgu8B96vq9935/wJQ1d+FqPsi8A9LBKeeil27KJgzhwNz5uLbt4+4jAxSr72WlDFX4U0OTzfONwe/YWneUpblL2P1rtX41FetC2lIpyG0iG0Rlm0b01RFKhFcA4xS1Zvd+euAwap6e4i6L1JHIhCRW4BbAM4888z+27ZtC0vMJnz85eUUvfMO+2fOomzdOjxJSaSMGUPqpEnEd8kI23YPlh/k39v/zdK8pby//f1AF9LA9gPJTnPOFjoldwrb9o1pKpp9IghmZwTNX+m6dRyYNYuDC99GKypocd55pE6eTHL2MMQbvu6bSn8ln+7+lOX5y1mat5Tcg7mA04VUdV3BupDMqcq6hkyTVLlvHwVz53IgZzaVu3cT27kzqddeS+uxV+NNCf8vjrcd3BboQlqzaw0+9dEmoQ3ndzqf4WnDOa/jedaFZE4ZkUoEMTgXi0cC23EuFl+rqutD1H0RSwRRSysqKHrvPfbPnEnpqtVIQgIpV1xB6uTJJJzd7aTEUHi40OlCyne6kIrKi4j1xDLwjIGBC84dkzuelFiMCYdI3j56Kc7toV7geVV9SEQeAFap6gIRGQi8AaQCZcC3qtqzrjYtEZzayr76iv0zZ3LwzX+ghw+TNHAgqZMn03LkBUjMyRkjsaoLaVmecxdSVRdS19SuZHfOti4k0yzZD8pMs+MrKKDgtdc48EoOFdu3E3PGGaROmEDrH4wjpk2bkxpLbmEuy/KXsTRvKZ/u/jTQhTS009BAF5KNrmqaOksEptlSn4/ipUs5MGsWhz74EImLo+WFI4lpfwaexAQkIRFPQgKSmIAnMcktS8CTmIgnMfHIdEICkpiIxMWd0GB2hYcLeX/7+yzLX1atC2nQGYP4XsfvkRzr3BJbtY3A8OBBai6rre5R9eqqX/VIjTrqypFK1YY3DywPEVfI9USqbafW9YLqBZfXXK+muvZJevSTO+ouP479m6LuM0aqnjniqzYfXK6qIZfXuqzqOSZ+X2A61Dq1tXvFWVcw8IyBDf5MYInAnCIOb97MgVmzKPrXu/gOHUJLS0M8f+EYPJ5AUqiWQKqmExKdZJKYiCchMagssfryhET8CbF8XbqNjw98xr/3rWJrWT5+Ab/zQAnnHeddg1847zSH0VVVEQWPgsdf472W8trqy1HrOrtvv0dQAT9HviO/p/r35q/lvWq63nU97mM4jvO7F9XQn6e2zx5iumadGBU86iEGIQYPXr/gxYNXweuWe1XwqHDBxVMZNfTG44vdnlBmTgXxZ53FGTNmcMaMGYBztKfl5fhLStCyMvylZWhZKf6yMvwlpc50aRn+slK0NHg6dJlv/wEqyna4y8vc5XUnmyRghPs6LiLgcTOHxxM07TxNzlnucY74Jeg9eLnI0fOBMncdvx98fuez+Hyo33+kzO8Hv7rv1culmR0o1pfW/N68nsB3rSLO5/b5q30n4o/8d3FGz8SwtGuJwDRbIoLEx4d8HkJjqUo2WlojwZSV4S8tdRJQSamTTMoOg/rdnaw6O131O90TfmcaVWe5An4/6pZVq1817fcD6sz73XWr1Q9uW4Pao3p9vyJeD3i8zrt4wOtBPN6gdy/ikSN1gpd5xPl9R9Cy2tvxOHXFU71u4N15VdU58pn9Rz6Tz3fk81Qr9zvfod8f+E7U7zuyPCihhS73VW+vWnnVd+y0LVVJtNrnC/F5atYJ+dmDPrOn9u/nyPdco47X62zb6yWmbduw/Du3RGBMHaqSDfHx2D1C5lTVuKN/GWOMaXYsERhjTJSzRGCMMVHOEoExxkQ5SwTGGBPlLBEYY0yUs0RgjDFRzhKBMcZEuWY31pCI7AGO91mVbYG9jRhOY7G4GsbiarimGpvF1TAnEtd3VLVdqAXNLhGcCBFZVdugS5FkcTWMxdVwTTU2i6thwhWXdQ0ZY0yUs0RgjDFRLtoSwdORDqAWFlfDWFwN11Rjs7gaJixxRdU1AmOMMUeLtjMCY4wxNVgiMMaYKBc1iUBERonIRhHZJCLTIx0PgIg8LyK7ReSLSMcSTETSRGSJiGwQkfUi8tNIxwQgIgki8omIfObG9ZtIxxRMRLwi8qmI/CPSsVQRkVwR+VxE1opIk3nYt4i0FpF5IvKViHwpIt9rAjGd7X5PVa+DIjIt0nEBiMhd7r/5L0QkR0QSGrX9aLhGICJe4D/ARUA+sBKYqKobIhzXMKAYeFlVe0UylmAi0gHooKprRKQlsBq4qgl8XwK0UNViEYkF3gd+qqofRTKuKiJyNzAAaKWql0c6HnASATBAVZvUj6NE5CVghao+KyJxQJKqFkQ6riruPmM7MFhVj/cHrI0VSyecf+vnqGqpiMwFFqrqi421jWg5IxgEbFLVLapaDswGroxwTKjqcmB/pOOoSVV3quoad7oI+BLoFNmoQB3F7mys+2oSRzIi0hm4DHg20rE0dSKSAgwDngNQ1fKmlARcI4HNkU4CQWKARBGJAZKAHY3ZeLQkgk5AXtB8Pk1gx9YciEg60Bf4OLKRONzul7XAbuBfqtok4gIeA/4f4I90IDUo8E8RWS0it0Q6GFcGsAd4we1Ke1ZEWkQ6qBomADmRDgJAVbcDjwDfADuBQlX9Z2NuI1oSgTkOIpIMvAZMU9WDkY4HQFV9qtoH6AwMEpGId6mJyOXAblVdHelYQjhfVfsBlwA/cbsjIy0G6Ac8pap9gUNAk7huB+B2VY0GXo10LAAikorTg5EBdARaiMjkxtxGtCSC7UBa0Hxnt8zUwu2Dfw2YpaqvRzqemtyuhCXAqEjHAgwBRrv98bOBC0RkZmRDcrhHk6jqbuANnG7SSMsH8oPO5ubhJIam4hJgjaruinQgrguBraq6R1UrgNeB8xpzA9GSCFYCXUUkw832E4AFEY6pyXIvyj4HfKmqj0Y6nioi0k5EWrvTiTgX/7+KbFSgqv+lqp1VNR3n39ZiVW3UI7bjISIt3Iv9uF0vFwMRv0NNVb8F8kTkbLdoJBDRGxFqmEgT6RZyfQOcKyJJ7v+bI3Gu2zWamMZsrKlS1UoRuR1YBHiB51V1fYTDQkRygOFAWxHJB+5T1eciGxXgHOFeB3zu9scD3KuqCyMYE0AH4CX3jg4PMFdVm8ytmk1Qe+ANZ99BDPCKqr4T2ZAC7gBmuQdmW4AbIxwPEEiYFwE/inQsVVT1YxGZB6wBKoFPaeShJqLi9lFjjDG1i5auIWOMMbWwRGCMMVHOEoExxkQ5SwTGGBPlLBEYY0yUs0RgTA0i4qsxCmWj/epVRNKb2mizxkTF7wiMaaBSdxgLY6KCnREYU0/u2P5/cMf3/0REvuuWp4vIYhFZJyLviciZbnl7EXnDfX7CZyJSNSyAV0SecceX/6f7K2ljIsYSgTFHS6zRNTQ+aFmhqmYCf8EZcRTgCeAlVe0NzAIed8sfB5apahbOWDpVv2bvCjypqj2BAmBsmD+PMXWyXxYbU4OIFKtqcojyXOACVd3iDsr3raqeJiJ7cR7kU+GW71TVtiKyB+isqoeD2kjHGT67qzv/CyBWVR8M/yczJjQ7IzCmYbSW6YY4HDTtw67VmQizRGBMw4wPev/Qnf4AZ9RRgEnACnf6PeA2CDxQJ+VkBWlMQ9iRiDFHSwwadRXgHVWtuoU0VUTW4RzVT3TL7sB52tY9OE/eqhpJ86fA0yLyQ5wj/9twnjBlTJNi1wiMqaem+iB4Y06UdQ0ZY0yUszMCY4yJcnZGYIwxUc4SgTHGRDlLBMYYE+UsERhjTJSzRGCMMVHu/wNVsHcoLgtM3QAAAABJRU5ErkJggg==\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.9"
    },
    "colab": {
      "name": "Models.ipynb",
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}